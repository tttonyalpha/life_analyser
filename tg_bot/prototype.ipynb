{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09122472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 13:10:36,369 - apscheduler.scheduler - INFO - Scheduler started\n",
      "2023-07-21 13:10:56,730 - telegram.ext.dispatcher - ERROR - No error handlers are registered, logging exception.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tonyalpha/.local/lib/python3.8/site-packages/telegram/ext/dispatcher.py\", line 557, in process_update\n",
      "    handler.handle_update(update, self, check, context)\n",
      "  File \"/home/tonyalpha/.local/lib/python3.8/site-packages/telegram/ext/handler.py\", line 199, in handle_update\n",
      "    return self.callback(update, context)\n",
      "  File \"<ipython-input-1-971e397ffdcf>\", line 22, in send_new_channel_messages\n",
      "    channel_updates = bot.get_chat_updates(channel_id)\n",
      "AttributeError: 'Bot' object has no attribute 'get_chat_updates'\n",
      "2023-07-21 13:11:06,016 - telegram.ext.updater - INFO - Received signal 2 (SIGINT), stopping...\n",
      "2023-07-21 13:11:06,019 - apscheduler.scheduler - INFO - Scheduler has been shut down\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "from telegram import Update, Bot\n",
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\n",
    "\n",
    "# Replace 'YOUR_BOT_TOKEN' with the token you got from the BotFather\n",
    "bot = Bot(token=API_TOKEN)\n",
    "updater = Updater(token=API_TOKEN, use_context=True)\n",
    "dispatcher = updater.dispatcher\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                     level=logging.INFO)\n",
    "\n",
    "\n",
    "def send_new_channel_messages(update: Update, context: CallbackContext):\n",
    "    # Replace 'YOUR_CHANNEL_ID' with the ID of the channel from which you want to read messages\n",
    "    channel_id = 'my life'\n",
    "\n",
    "    # Get the list of all updates in the channel\n",
    "    channel_updates = bot.get_chat_updates(channel_id)\n",
    "\n",
    "    # Get the user ID to send messages\n",
    "    user_id = update.message.from_user.id\n",
    "\n",
    "    # Iterate through all the updates and send new messages to the user\n",
    "    for update in channel_updates:\n",
    "        if update.message:\n",
    "            bot.forward_message(chat_id=user_id, from_chat_id=channel_id, message_id=update.message.message_id)\n",
    "\n",
    "\n",
    "def start(update: Update, context: CallbackContext):\n",
    "    user_id = update.message.from_user.id\n",
    "    update.message.reply_text(f\"Hello! I will send you new messages from the channel.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Add a handler for new messages in the channel\n",
    "    dispatcher.add_handler(MessageHandler(Filters.chat_type.channel, send_new_channel_messages))\n",
    "    # Add a handler for the /start command\n",
    "    dispatcher.add_handler(CommandHandler(\"start\", start))\n",
    "\n",
    "    updater.start_polling()\n",
    "    updater.idle()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb7e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d39899cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-192-20c5fbbb96dc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-192-20c5fbbb96dc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    a[2]['text'].\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a[2]['text']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6424730",
   "metadata": {},
   "source": [
    "# Config upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83d4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "\n",
    "from telethon import TelegramClient\n",
    "from telethon.errors import SessionPasswordNeededError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a02dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Configs\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# Setting configuration values\n",
    "api_id = config['Telegram']['api_id']\n",
    "api_hash = config['Telegram']['api_hash']\n",
    "api_token = config['Telegram']['api_token']\n",
    "\n",
    "api_hash = str(api_hash)\n",
    "\n",
    "phone = config['Telegram']['phone']\n",
    "username = config['Telegram']['username']\n",
    "channel_link = config['Telegram']['channel_link'] \n",
    "\n",
    "db_name = config['database']['db_name']\n",
    "db_user = config['database']['db_user']\n",
    "db_password = config['database']['db_password'] \n",
    "db_host = config['database']['db_host'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e06c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "32679bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<coroutine object UserMethods.get_me at 0x7f86fd106540>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-258-bc2b6df2013c>:1: RuntimeWarning: coroutine 'UserMethods.is_user_authorized' was never awaited\n",
      "  if not client.is_user_authorized():\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "<ipython-input-258-bc2b6df2013c>:8: RuntimeWarning: coroutine 'UserMethods.get_me' was never awaited\n",
      "  me = client.get_me()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "if not client.is_user_authorized():\n",
    "    client.send_code_request(phone)\n",
    "    try:\n",
    "        client.sign_in(phone, input('Enter the code: '))\n",
    "    except SessionPasswordNeededError:\n",
    "        client.sign_in(password=input('Password: '))\n",
    "\n",
    "me = client.get_me()\n",
    "print(me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92a4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49f11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-adc2bb114a1f>:11: RuntimeWarning: coroutine 'TelegramBaseClient.connect' was never awaited\n",
      "  assert client.connect()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "<ipython-input-3-adc2bb114a1f>:12: RuntimeWarning: coroutine 'UserMethods.is_user_authorized' was never awaited\n",
      "  if not client.is_user_authorized():\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "chat_id = 'my life'\n",
    "\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "import telethon.sync\n",
    "\n",
    "client = TelegramClient(None,\n",
    "                    api_id,\n",
    "                    api_hash)\n",
    "\n",
    "assert client.connect()\n",
    "if not client.is_user_authorized():\n",
    "    client.send_code_request(phone_number)\n",
    "    me = client.sign_in(phone_number, input('Enter code: '))\n",
    "\n",
    "channel_username='my life' # your channel\n",
    "channel_entity=client.get_entity(channel_username)\n",
    "posts = client(GetHistoryRequest(\n",
    "    peer=channel_entity,\n",
    "    limit=100,\n",
    "    offset_date=None,\n",
    "    offset_id=0,\n",
    "    max_id=0,\n",
    "    min_id=0,\n",
    "    add_offset=0,\n",
    "    hash=0))\n",
    "# messages stored in `posts.messages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce2577eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-e72b7ee019a6>:4: RuntimeWarning: coroutine 'AuthMethods._start' was never awaited\n",
      "  client.start()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "<ipython-input-21-e72b7ee019a6>:6: RuntimeWarning: coroutine 'MessageMethods.get_messages' was never awaited\n",
      "  for message in client.get_messages(channel_username, limit=10):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'coroutine' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e72b7ee019a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchannel_username\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'my life'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_username\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'coroutine' object is not iterable"
     ]
    }
   ],
   "source": [
    "from telethon import TelegramClient, events, sync\n",
    "\n",
    "client = TelegramClient(None, api_id, api_hash)\n",
    "client.start()\n",
    "channel_username = 'my life'\n",
    "for message in client.get_messages(channel_username, limit=10):\n",
    "    print(message.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ef6b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.sync import TelegramClient\n",
    "\n",
    "\n",
    "\n",
    "# We have to manually call \"start\" if we want an explicit bot token\n",
    "bot = TelegramClient('bot', api_id, api_hash).start(bot_token=api_token)\n",
    "\n",
    "# But then we can use the client instance as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2052f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.tl.functions.channels import GetParticipantsRequest\n",
    "from telethon.tl.types import ChannelParticipantsSearch\n",
    "from telethon.tl.types import (\n",
    "PeerChannel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2f31b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter entity(telegram URL or entity id):https://web.telegram.org/k/#-1810810362\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input_channel = input(\"enter entity(telegram URL or entity id):\")\n",
    "\n",
    "if user_input_channel.isdigit():\n",
    "    entity = PeerChannel(int(user_input_channel))\n",
    "else:\n",
    "    entity = user_input_channel\n",
    "\n",
    "my_channel = client.get_entity(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d767a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'users'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-260026df92e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ))\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparticipants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mall_participants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticipants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'users'"
     ]
    }
   ],
   "source": [
    "\n",
    "offset = 0\n",
    "limit = 100\n",
    "all_participants = []\n",
    "\n",
    "while True:\n",
    "    participants = client(GetParticipantsRequest(\n",
    "        my_channel, ChannelParticipantsSearch(''), offset, limit,\n",
    "        hash=0\n",
    "    ))\n",
    "    if not participants.users:\n",
    "        break\n",
    "    all_participants.extend(participants.users)\n",
    "    offset += len(participants.users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28aa8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.tl.functions.messages import (GetHistoryRequest)\n",
    "from telethon.tl.types import (\n",
    "PeerChannel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b17d1cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Offset ID is: 0 ; Total Messages: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3a17a70c97d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     ))\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'messages'"
     ]
    }
   ],
   "source": [
    "\n",
    "offset_id = 0\n",
    "limit = 100\n",
    "all_messages = []\n",
    "total_messages = 0\n",
    "total_count_limit = 0\n",
    "\n",
    "while True:\n",
    "    print(\"Current Offset ID is:\", offset_id, \"; Total Messages:\", total_messages)\n",
    "    history = client(GetHistoryRequest(\n",
    "        peer=my_channel,\n",
    "        offset_id=offset_id,\n",
    "        offset_date=None,\n",
    "        add_offset=0,\n",
    "        limit=limit,\n",
    "        max_id=0,\n",
    "        min_id=0,\n",
    "        hash=0\n",
    "    ))\n",
    "    if not history.messages:\n",
    "        break\n",
    "    messages = history.messages\n",
    "    for message in messages:\n",
    "        all_messages.append(message.to_dict())\n",
    "    offset_id = messages[len(messages) - 1].id\n",
    "    total_messages = len(all_messages)\n",
    "    if total_count_limit != 0 and total_messages >= total_count_limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a2cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-ac3a12e7944d>:8: RuntimeWarning: coroutine 'AuthMethods._start' was never awaited\n",
      "  bot.start(bot_token=api_token)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future pending cb=[shield.<locals>._outer_done_callback() at /usr/lib/python3.8/asyncio/tasks.py:902]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl import functions, types\n",
    "\n",
    "\n",
    "\n",
    "bot = TelegramClient(None, api_id, api_hash)\n",
    "bot.start(bot_token=api_token)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    channel = await client.get_entity('my life')\n",
    "    messages = await client.get_messages(channel, limit= None) #pass your own args\n",
    "\n",
    "    #then if you want to get all the messages text\n",
    "    for x in messages:\n",
    "        print(x.text) #return message.text\n",
    "\n",
    "        \n",
    "bot.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0604599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2023, 7, 20)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date.today() - datetime.timedelta(days=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b8a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32ad8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon.tl.types import InputMessagesFilterPhotos\n",
    "from telethon import TelegramClient, events\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os.path\n",
    "\n",
    "def get_posts(num_posts = 1, download_media = False, only_with_media = True, offset_id = 0, posts_list = []):\n",
    "    client = TelegramClient(username, api_id, api_hash)\n",
    "    posts_list = []\n",
    "    async def main(num_posts, download_media, offset_id, posts_list):\n",
    "        \n",
    "        if num_posts < 100:\n",
    "            message_limit = num_posts\n",
    "        else:\n",
    "            message_limit = 100\n",
    "            \n",
    "            \n",
    "        last_post_date = None\n",
    "        post = {}\n",
    "        \n",
    "        await client.start()\n",
    "\n",
    "        try:\n",
    "            entity = await client.get_entity(channel_link)\n",
    "\n",
    "            while True:\n",
    "                messages = await client.get_messages(entity, limit=message_limit, offset_id = offset_id)\n",
    "\n",
    "                for message in messages:\n",
    "                    \n",
    "                    if only_with_media:\n",
    "                        if not message.media:\n",
    "                            continue\n",
    "                    \n",
    "                    if last_post_date == None:\n",
    "                        last_post_date = message.date\n",
    "\n",
    "\n",
    "                    if last_post_date != message.date: \n",
    "                        post['upload_date'] = last_post_date.strftime('%Y-%m-%d %H:%M:%S') if last_post_date else None\n",
    "\n",
    "                        last_post_date = message.date\n",
    "\n",
    "                        posts_list.append(post)\n",
    "                        post = {}\n",
    "\n",
    "\n",
    "                    if len(message.message) > 0:\n",
    "                        post['text'] = post.get('text', '') + message.message \n",
    "                        \n",
    "                        # post_id in messages set will be id of message with text and photo \n",
    "                        post['post_id'] = message.id\n",
    "\n",
    "\n",
    "                    if message.media and download_media:\n",
    "\n",
    "                        photo_id = message.media.photo.id\n",
    "                        filename = f'image_{photo_id}.jpg'\n",
    "                        path = f\"./media/{filename}\"\n",
    "\n",
    "                        # some of this i should drop, but later \n",
    "                        post['photos_id_list'] = post.get('photos_id_list', []) + [photo_id]\n",
    "                        post['photos_names_list'] = post.get('photos_names_list', []) + [filename]\n",
    "\n",
    "                        #if file exist check \n",
    "                        if not os.path.isfile(path) and download_media:\n",
    "                            await client.download_media(message, file=path)\n",
    "\n",
    "                    post['id_list'] = post.get('id_list', []) + [message.id]\n",
    "                    post['edit_date'] = message.edit_date.strftime('%Y-%m-%d %H:%M:%S') if message.edit_date else None\n",
    "\n",
    "\n",
    "                offset_id = messages[len(messages) - 1].id\n",
    "\n",
    "                if len(posts_list) >= num_posts:\n",
    "                    break\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        await client.disconnect()\n",
    "        \n",
    "        return posts_list\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        client.loop.run_until_complete(main(num_posts, download_media, offset_id, posts_list))\n",
    "        \n",
    "    return posts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "43eebfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'asdfsdf',\n",
       "  'post_id': 700,\n",
       "  'id_list': [700],\n",
       "  'edit_date': None,\n",
       "  'upload_date': '2023-07-24 14:39:42'}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_posts(num_posts = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34369a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914015df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05.04.23\\n\\nüë®\\u200düë®\\u200düë¶\\u200düë¶ ü§¶üèº Did not find team for HSE hackaton\\nüíÅüèº\\u200d‚ôÇÔ∏èüí¨ ODS random coffee with Roman\\nüìö DL: semianr\\n\\nDay rate: 5.5/10'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedb9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "f41d676c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['—á–ø–æ–∫',\n",
       " '',\n",
       " 'How productive have you been?:6.999/10',\n",
       " 'How interesting was the day?: 6/10',\n",
       " 'How stressful was the day?: 2/10']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " a[0]['text'].split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d073d9",
   "metadata": {},
   "source": [
    "# Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60bc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e844737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f799c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "\n",
    "def parse_text(dct):\n",
    "    \n",
    "    input_string = dct['text']\n",
    "    print(input_string)\n",
    "    \n",
    "    date_pattern = r'\\d{2}\\.\\d{2}\\.\\d{2}'\n",
    "    day_pattern = r'\\b(?:MONDAY|TUESDAY|WEDNESDAY|THURSDAY|FRIDAY|SATURDAY|SUNDAY)\\b'\n",
    "    \n",
    "    date_match = re.search(date_pattern, input_string)\n",
    "    day_match = re.search(day_pattern, input_string)\n",
    "    \n",
    "    date = date_match.group() if date_match else None\n",
    "    day = day_match.group() if day_match else None\n",
    "    \n",
    "    parsed_date = date.replace('.', '-') if date or day else None\n",
    "    \n",
    "    dct['parsed_date'] = parsed_date\n",
    "    \n",
    "    if date_match:\n",
    "        input_string = input_string.replace(date_match.group(), \"\") if parsed_date is not None else input_string\n",
    "    if day_match:\n",
    "        input_string = input_string.replace(day_match.group(), \"\") if parsed_date is not None else input_string\n",
    "    \n",
    "    regex_list = [r'How productive have you been\\?:\\s*(\\d+\\.?\\d*)/10', r'How interesting was the day\\?:\\s*(\\d+\\.?\\d*)/10', r'How stressful was the day\\?:\\s*(\\d+\\.?\\d*)/10']\n",
    "    names_list = ['productivity_score', 'interest_score', 'stress_score']\n",
    "    \n",
    "    \n",
    "    for regex, name in zip(regex_list, names_list):\n",
    "        match = re.search(regex, input_string)\n",
    "        parsed_score = float(match.group(1)) if match else None\n",
    "        \n",
    "        dct[name] = parsed_score\n",
    "\n",
    "        input_string = input_string.replace(match.group(), \"\") if parsed_score is not None else input_string\n",
    "        \n",
    "    \n",
    "    dct['parsed_text'] = input_string.split('\\n')\n",
    "    \n",
    "    print('Text parsed!')\n",
    "    return dct\n",
    "    \n",
    "def parse_activities(dct):\n",
    "    \n",
    "    input_list = dct['parsed_text']\n",
    "    \n",
    "    result = []\n",
    "    for item in input_list:\n",
    "        match = re.match(r'^([\\U0001F000-\\U0001F9FF]+|[\\U0001FA00-\\U0001FA6F]+)?\\s*(.*)$', item)\n",
    "        if match:\n",
    "            emoji = match.group(1)\n",
    "            text = match.group(2).strip()\n",
    "            if emoji or text:\n",
    "                result.append((emoji, text))\n",
    "                \n",
    "    dct['parsed_activities'] = result\n",
    "    \n",
    "    print('Activities parsed!')\n",
    "    return dct\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f990c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90bf6246774a71bd385281a4189dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee097d91c7e432a827986b946ac9187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", return_all_scores=True)\n",
    "classifier(\"I love this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb467592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2b2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736c9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "636147aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.06.23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id_list': [564, 563, 562, 561],\n",
       " 'edit_date': None,\n",
       " 'text': '24.06.23',\n",
       " 'post_id': 561,\n",
       " 'upload_date': '2023-06-28 15:43:13',\n",
       " 'parsed_date': '24-06-23',\n",
       " 'productivity_score': None,\n",
       " 'interest_score': None,\n",
       " 'stress_score': None,\n",
       " 'parsed_text': [''],\n",
       " 'parsed_activities': []}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_activities(parse_text(a[30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "045bc81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_list': [564, 563, 562, 561],\n",
       " 'edit_date': None,\n",
       " 'text': '24.06.23',\n",
       " 'post_id': 561,\n",
       " 'upload_date': '2023-06-28 15:43:13'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "b8ea6221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24.05.23\\n\\n* IAD: control work \\n\\n* My project for resume \\n* My project: life analyze\\n\\n* English speaking club \\n\\n* Optics: think about future....\\n\\nHow productive have you been?: */10\\nHow interesting was the day?: */10\\nHow stressful was the day?: */10\\nPAI: *'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[79]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b809a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b4c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66c02769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abf4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f3ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd1ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a77d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5cff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc691bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd984c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f584c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bf9c19c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.01.23\n",
      "16.02.23\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593f9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2bf5c57",
   "metadata": {},
   "source": [
    "# Upload to DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592655d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28dc6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8066214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to database successfully\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa3700bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72416713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "cursor.execute(\"SELECT * FROM pg_catalog.pg_tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78cb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba714d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e742f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "from getpass import getpass\n",
    "from mysql.connector import connect, Error\n",
    "\n",
    "\n",
    "def db_connect():\n",
    "    try:\n",
    "        with connect(host=db_host, user=db_user, password=db_password,) as connection:\n",
    "            use_db_query = f\"USE {db_name};\"\n",
    "            create_posts_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS posts(\n",
    "                post_id INT,\n",
    "                text TEXT,\n",
    "                parsed_date DATE,\n",
    "                upload_date DATETIME,\n",
    "                edit_date DATETIME, \n",
    "                productivity_score FLOAT,\n",
    "                interest_score FLOAT,\n",
    "                stress_score FLOAT,\n",
    "                PRIMARY KEY (post_id)\n",
    "            );\n",
    "            \"\"\"\n",
    "            \n",
    "            create_posts_table_activities = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS activities(\n",
    "                activity_id INT AUTO_INCREMENT,\n",
    "                post_id INT,\n",
    "                activity_name TEXT,\n",
    "                activity_emoji TEXT,\n",
    "                activity_type TEXT,\n",
    "                activity_emotion TEXT,\n",
    "                \n",
    "                FOREIGN KEY (post_id) REFERENCES posts(post_id),\n",
    "                PRIMARY KEY (activity_id)\n",
    "            );\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(use_db_query)\n",
    "                cursor.execute(create_posts_table_query)\n",
    "                cursor.execute(create_posts_table_activities)\n",
    "                connection.commit()\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def upload_post(dct):\n",
    "    try:\n",
    "        with connect(host=db_host, user=db_user, password=db_password,) as connection:\n",
    "            use_db_query = f\"USE {db_name};\"\n",
    "            check_if_exist = \"\"\"SELECT EXISTS(SELECT 1 FROM posts WHERE post_id = %s LIMIT 1)\"\"\"\n",
    "            \n",
    "            upload_posts_table_query = f\"\"\"\n",
    "            INSERT INTO posts (post_id, text, parsed_date, upload_date, \n",
    "            edit_date,productivity_score, interest_score, stress_score) \n",
    "            VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            post_id = (dct.get('post_id', None),)\n",
    "            \n",
    "            insert_data = (dct.get('post_id', None), dct.get('text', None), \n",
    "                    dct.get('parsed_date', None), dct.get('upload_date', None), \n",
    "                    dct.get('edit_date', None),dct.get('productivity_score', None),\n",
    "                    dct.get('interest_score', None),dct.get('stress_score', None))\n",
    "            \n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(use_db_query)\n",
    "                cursor.execute(check_if_exist, post_id)\n",
    "                for el in cursor:\n",
    "                    exist = el[0]\n",
    "                \n",
    "                if not exist:\n",
    "                    cursor.execute(upload_posts_table_query, insert_data)\n",
    "                    connection.commit()\n",
    "                else:\n",
    "                    print('ERROR: post already uploaded, if you need update it use update_post()')\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def upload_activities(dct):\n",
    "    try:\n",
    "        with connect(host=db_host, user=db_user, password=db_password,) as connection:\n",
    "            use_db_query = f\"USE {db_name};\"\n",
    "            check_if_exist = \"\"\"SELECT EXISTS(SELECT 1 FROM posts WHERE post_id = %s LIMIT 1)\"\"\"\n",
    "            \n",
    "            upload_posts_table_query = f\"\"\"\n",
    "            INSERT INTO posts (post_id, text, parsed_date, upload_date, \n",
    "            edit_date,productivity_score, interest_score, stress_score) \n",
    "            VALUES(%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            post_id = (dct.get('post_id', None),)\n",
    "            \n",
    "            insert_data = (dct.get('post_id', None), dct.get('text', None), \n",
    "                    dct.get('parsed_date', None), dct.get('upload_date', None), \n",
    "                    dct.get('edit_date', None),dct.get('productivity_score', None),\n",
    "                    dct.get('interest_score', None),dct.get('stress_score', None))\n",
    "            \n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(use_db_query)\n",
    "                cursor.execute(check_if_exist, post_id)\n",
    "                for el in cursor:\n",
    "                    exist = el[0]\n",
    "                \n",
    "                if not exist:\n",
    "                    cursor.execute(upload_posts_table_query, insert_data)\n",
    "                    connection.commit()\n",
    "                else:\n",
    "                    print('ERROR: post already uploaded, if you need update it use update_post()')\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def update_post(dct):\n",
    "    try:\n",
    "        with connect(host=db_host, user=db_user, password=db_password,) as connection:\n",
    "            use_db_query = f\"USE {db_name};\"\n",
    "            update_posts_table_query = \"\"\"\n",
    "            UPDATE posts SET text = %s, parsed_date = %s,  \n",
    "            upload_date = %s, edit_date = %s, productivity_score = %s,\n",
    "            interest_score = %s, stress_score = %s WHERE post_id = %s\n",
    "            \"\"\"\n",
    "            \n",
    "            data = (dct.get('text', None), \n",
    "                    dct.get('parsed_date', None), dct.get('upload_date', None), \n",
    "                    dct.get('edit_date', None),dct.get('productivity_score', None),\n",
    "                    dct.get('interest_score', None),dct.get('stress_score', None),\n",
    "                   dct.get('post_id', None))\n",
    "\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(use_db_query)\n",
    "                cursor.execute(update_posts_table_query, data)\n",
    "                connection.commit()\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "def db_show_tables():\n",
    "    try:\n",
    "        with connect(host=db_host, user=db_user, password=db_password,) as connection:\n",
    "            use_db_query = f\"USE {db_name};\"\n",
    "            show_tables = \"SHOW TABLES;\"\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(use_db_query)\n",
    "                cursor.execute(show_tables)\n",
    "                for el in cursor.fetchall():\n",
    "                    print(el)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def db_execute_query(query):\n",
    "    try:\n",
    "        with connect(host=db_host, user=db_user, password=db_password,) as connection:\n",
    "            use_db_query = f\"USE {db_name};\"\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(use_db_query)\n",
    "                cursor.execute(query)\n",
    "                for el in cursor:\n",
    "                    print(el)\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c866544",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6a44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "04f39dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('posts',)\n"
     ]
    }
   ],
   "source": [
    "db_show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "99624d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "db_execute_query('SELECT EXISTS(SELECT * FROM posts WHERE post_id = 9 LIMIT 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e91549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_execute_query('SELECT * FROM posts;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad303e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac9320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb81cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b87db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4938af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167f6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a43b130d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bb4e3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_post(post):\n",
    "    \n",
    "    date = post.get('upload_date', None)\n",
    "    \n",
    "    if post.get('parsed_date', None) is None:\n",
    "        print(f'I dont see date, can you add it to post uploaded at {date}?')\n",
    "        return False\n",
    "\n",
    "    if post.get('productivity_score', None) is None:\n",
    "        print(f'I dont see productivity_score, can you add it to post uploaded at {date}?')\n",
    "        return False\n",
    "        \n",
    "    if post.get('interest_score', None) is None:\n",
    "        print(f'I dont see interest_score, can you add it to post uploaded at {date}?')\n",
    "        return False\n",
    "        \n",
    "    if post.get('productivity_score', None) is None:\n",
    "        print(f'I dont see productivity_score, can you add it to post uploaded at {date}?')\n",
    "        return False\n",
    "        \n",
    "    if post.get('stress_score', None) is None:\n",
    "        print(f'I dont see stress_score, can you add it to postuploaded at {date}?')\n",
    "        return False\n",
    "    print('Post correct!')\n",
    "    return True\n",
    "\n",
    "def check_activities(post):\n",
    "    \n",
    "    date = post.get('upload_date', None)\n",
    "    \n",
    "    if post.get('parsed_activities', None) is None or len(post.get('parsed_activities', None)) < 0:\n",
    "        print(f'I dont see activities, can you add some to post uploaded at {date}?')\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3c6391fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = get_posts(num_posts = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bccb069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '05.04.23\\nasdfsdf\\n\\nHow productive have you been?: 0/10\\nHow interesting was the day?: 9/10\\nHow stressful was the day?: 0/10',\n",
       "  'post_id': 700,\n",
       "  'id_list': [700],\n",
       "  'edit_date': '2023-07-24 15:57:47',\n",
       "  'upload_date': '2023-07-24 14:39:42'}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "857bc223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05.04.23\n",
      "asdfsdf\n",
      "\n",
      "How productive have you been?: 0/10\n",
      "How interesting was the day?: 9/10\n",
      "How stressful was the day?: 0/10\n",
      "Text parsed!\n",
      "Activities parsed!\n",
      "Post correct!\n",
      "ERROR: post already uploaded, if you need update it use update_post()\n",
      "—á–ø–æ–∫\n",
      "\n",
      "How productive have you been?:6.999/10\n",
      "How interesting was the day?: 6/10\n",
      "How stressful was the day?: 2/10\n",
      "Text parsed!\n",
      "Activities parsed!\n",
      "I dont see date, can you add it to post uploaded at 2023-07-23 16:22:13?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "posts = get_posts(num_posts = 2)\n",
    "\n",
    "for post in posts:\n",
    "    post = parse_text(post)\n",
    "    post = parse_activities(post)\n",
    "    if check_post(post):\n",
    "        upload_post(post)\n",
    "    \n",
    "    if check_activities(post):\n",
    "        upload_activities(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716acc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb18d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee90d36e",
   "metadata": {},
   "source": [
    "# Activity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ff0ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d95fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"/media/tonyalpha/HDD/facebook-bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c1cb54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Evening cycling to grocery',\n",
       " 'labels': ['travel_&_adventure',\n",
       "  'sport_&_health',\n",
       "  'selfdev',\n",
       "  'chore',\n",
       "  'work',\n",
       "  'relax_&_rest',\n",
       "  'family',\n",
       "  'bad habit',\n",
       "  'university'],\n",
       " 'scores': [0.40217676758766174,\n",
       "  0.18061843514442444,\n",
       "  0.16332343220710754,\n",
       "  0.10840290784835815,\n",
       "  0.06324637681245804,\n",
       "  0.02999110333621502,\n",
       "  0.025648843497037888,\n",
       "  0.019720228388905525,\n",
       "  0.0068719638511538506]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"Evening cycling to grocery\"\n",
    "candidate_labels = ['selfdev', 'university', 'work', 'relax_&_rest', 'sport_&_health', 'family', 'travel_&_adventure', 'chore', 'bad habit']\n",
    "classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5b00c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab579dd9",
   "metadata": {},
   "source": [
    "# Sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deec4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"/media/tonyalpha/HDD/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "382fa658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEG', 'score': 0.8925472497940063},\n",
       " {'label': 'NEU', 'score': 0.0853690579533577},\n",
       " {'label': 'POS', 'score': 0.022083677351474762}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('Sad day', top_k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792aabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d7aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4be5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cac40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af632f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412958a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7252baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03f48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07697855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b292211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a9d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b84e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fe04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6dafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d61c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f8e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ed7682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12da09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99dec74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfcdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65c377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23a564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8d70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "743a5c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf06b60357b449aa3989e5a2d4f2e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e513cdf44054a55b8996fca5b1e574d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b69fcc109e49898727f7e37dcdf8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b691bfe0e5494e9bf80942152a69c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/196M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a254b4a65604b8aadaaf1f14cb2f1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87d9c09e5064d2fa4768a5550e95137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984c2b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662b4baaf27e4232a0dbe97d4dd52e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd082cccd5ee4d4aab3295c7282ac363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f952412163f44774a4f7dee4156c9c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3455aee3059c46118e65104c4323edb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cc464ef1694d6bbedf9dfe254adf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4142f7dfd54e4f10b957a6236b309a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caeaee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b5e672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853c8b23f9594586b68fa39fdb65d358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cdd4dd3d4f8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-cased\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    472\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m                         \u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m                     }\n\u001b[0;32m-> 2450\u001b[0;31m                     \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcached_file_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m                     \u001b[0;31m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa93e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a272d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08cba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e463f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f8941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88127196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "500723f2",
   "metadata": {},
   "source": [
    "# Sync with DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e069f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5303c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d55939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a13af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e02ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5d0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6ed0941",
   "metadata": {},
   "source": [
    "# Etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c54f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
